{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Psychsim\n",
    "Psychsim is a tool for constructing and simulating social scenarios rapidly. It is an implementation of the Partially Observable Markov Decision Process (POMDP), and allows rational agents to make decisions. In this tutorial, we build a maze in a 2D grid world, and create an agent that can solve the maze. The maze is 5 tiles by 5 tiles in size with many obstacles blocking the path.\n",
    "\n",
    "This scenario can be visualized by running demo.py code, which requires the pyglet graphics library.\n",
    "\n",
    "\n",
    "Begin by importing the Psychsim modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from psychsim.reward import *\n",
    "from psychsim.pwl import *\n",
    "from psychsim.action import *\n",
    "from psychsim.world import *\n",
    "from psychsim.agent import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Agent\n",
    "First, define and create a world as the setting for the scenario. Then, create some agent, give it a name, and set the number of steps it looks ahead (horizon). That agent has to be added to the world to act."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Agent']\n"
     ]
    }
   ],
   "source": [
    "world = World()\n",
    "actor = Agent('Agent')\n",
    "actor.setHorizon(5)\n",
    "world.addAgent(actor)\n",
    "print(world.agents.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Agent States\n",
    "An agent can have states, which can be defined in the world with the agent's name, the state's name, and the type. In the grid world, we define the agent's x and y coordinate as states, and the coordinate of the end of the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "world.defineState(actor.name,'x',int)\n",
    "world.defineState(actor.name,'y',int)\n",
    "world.defineState(actor.name, 'goal_x', int)\n",
    "world.defineState(actor.name, 'goal_y', int);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the initial values for the states, and access them. In a fully-observable deterministic model, probability of a state is 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 100%\t0 , y: 100%\t0\n",
      "goal x: 100%\t5 , goal y: 100%\t5\n"
     ]
    }
   ],
   "source": [
    "world.setState(actor.name, 'x', 0)\n",
    "world.setState(actor.name, 'y', 0)\n",
    "world.setState(actor.name, 'goal_x', 5)\n",
    "world.setState(actor.name, 'goal_y', 5)\n",
    "\n",
    "print('x: ' + str(world.getState(actor.name,'x')) + ' , y: ' + str(world.getState(actor.name,'y')))\n",
    "print('goal x: ' + str(world.getState(actor.name,'goal_x')) + ' , goal y: ' + str(world.getState(actor.name,'goal_y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a list of obstacles in the maze that the agent needs to move around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obstacles = [(0,1),(0,2),(0,3),(1,3),(2,3),(3,3),(4,3),(4,2),(4,1),(2,1),(2,0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Agent Actions\n",
    "In the grid world, the agent can only move right, left, up, and down. We can add these actions to the agent. Additionally, each action has some effect on the world dynamics. For example, if the agent moves right, its x-coordinate increases by one. We have to create a decision tree for these actions and their effect on the world for the decision-making process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "move_right = actor.addAction({'verb': 'MoveRight'})\n",
    "tree = makeTree(incrementMatrix(stateKey(move_right['subject'], 'x'), 1.))\n",
    "world.setDynamics(stateKey(move_right['subject'], 'x'), move_right, tree)\n",
    "\n",
    "move_left = actor.addAction({'verb': 'MoveLeft'})\n",
    "tree = makeTree(incrementMatrix(stateKey(move_left['subject'], 'x'), -1.))\n",
    "world.setDynamics(stateKey(move_left['subject'], 'x'), move_left, tree)\n",
    "\n",
    "move_up = actor.addAction({'verb': 'MoveUp'})\n",
    "tree = makeTree(incrementMatrix(stateKey(move_up['subject'], 'y'), 1.))\n",
    "world.setDynamics(stateKey(move_up['subject'], 'y'), move_up, tree)\n",
    "\n",
    "move_down = actor.addAction({'verb': 'MoveDown'})\n",
    "tree = makeTree(incrementMatrix(stateKey(move_down['subject'], 'y'), -1.))\n",
    "world.setDynamics(stateKey(move_down['subject'], 'y'), move_down, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Legality of Actions\n",
    "Next, we have to determine the legality of the agent's actions when it comes to obstacles. The agent cannot move outside of the map's boundaries and certainly not onto the obstacles themselves. To set the legality of these actions, we need to build a decision tree that checks the agent's states compared to some value. The tree is the legality of the \"move right\" action, with respect to two obstacles on the map. With an increased number of obstacles, the trees get more complicated.\n",
    "<img src=\"files/decisiontree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive function calls that check the x and y values of the agent's current location against the list of obstacles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_branch_plus_x(i):\n",
    "    if i == -1:\n",
    "        return True\n",
    "    else:\n",
    "        return {'if': equalRow(stateKey(actor.name, 'x'), obstacles[i][0]-1), True: {'if': equalRow(stateKey(actor.name, 'y'), obstacles[i][1]),True: False, False: add_branch_plus_x(i-1)}, False: add_branch_plus_x(i-1)}\n",
    " \n",
    "def add_branch_minus_x(i):\n",
    "    if i == -1:\n",
    "        return True\n",
    "    else:\n",
    "        return {'if': equalRow(stateKey(actor.name, 'x'), obstacles[i][0]+1), True: {'if': equalRow(stateKey(actor.name, 'y'), obstacles[i][1]),True: False, False: add_branch_minus_x(i-1)}, False: add_branch_minus_x(i-1)}\n",
    "\n",
    "def add_branch_plus_y(i):\n",
    "    if i == -1:\n",
    "        return True\n",
    "    else:\n",
    "        return {'if': equalRow(stateKey(actor.name, 'y'), obstacles[i][1]-1), True: {'if': equalRow(stateKey(actor.name, 'x'), obstacles[i][0]),True: False, False: add_branch_plus_y(i-1)}, False: add_branch_plus_y(i-1)}\n",
    "\n",
    "def add_branch_minus_y(i):\n",
    "    if i == -1:\n",
    "        return True\n",
    "    else:\n",
    "        return {'if': equalRow(stateKey(actor.name, 'y'), obstacles[i][1]+1), True: {'if': equalRow(stateKey(actor.name, 'x'), obstacles[i][0]),True: False, False: add_branch_minus_y(i-1)}, False: add_branch_minus_y(i-1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving right is invalid if:<br>\n",
    "1) agent's x+1 = obstacle's x and agent's y = obstacle's y<br>\n",
    "2) agent's x = 5, which is the rightmost map boundary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = makeTree({'if': equalRow(stateKey(actor.name, 'x'), '5'),\n",
    "             True: False, False: add_branch_plus_x(len(obstacles)-1)})\n",
    "actor.setLegal(move_right, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving left is invalid if:<br>\n",
    "1) agent's x-1 = obstacle's x and agent's y = obstacle's y<br>\n",
    "2) agent's x = 0, which is the leftmost map boundary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = makeTree({'if': equalRow(stateKey(actor.name, 'x'), '0'),\n",
    "             True: False, False: add_branch_minus_x(len(obstacles)-1)})\n",
    "actor.setLegal(move_left, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving up is invalid if:<br>\n",
    "1) agent's y+1 = obstacle's y and agent's x = obstacle's x<br>\n",
    "2) agent's y = 5, which is the topmost map boundary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = makeTree({'if': equalRow(stateKey(actor.name, 'y'), '5'),\n",
    "             True: False, False: add_branch_plus_y(len(obstacles)-1)})\n",
    "actor.setLegal(move_up, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving down is invalid if:<br>\n",
    "1) agent's y-1 = obstacle's y and agent's x = obstacle's x<br>\n",
    "2) agent's y = 0, which is the bottommost map boundary.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = makeTree({'if': equalRow(stateKey(actor.name, 'y'), '0'),\n",
    "             True: False, False: add_branch_minus_y(len(obstacles)-1)})\n",
    "actor.setLegal(move_down, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the Agent Reward Function\n",
    "The rationale for agents to act is on their reward functions. In this example, we take the L1 distance between the agent and the end of the maze as the reward function. The agent's only reward is to minimize the L1 distance, making this a greedy algorithm to reach the end of the maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actor.setReward(minimizeDifference(stateKey(actor.name, 'x'), stateKey(actor.name, 'goal_x')), 1.0)\n",
    "actor.setReward(minimizeDifference(stateKey(actor.name, 'y'), stateKey(actor.name, 'goal_y')), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Termination\n",
    "When the agent successfully reaches the end of the maze, the world needs to terminate. To create a termination condition, we need to create a decision tree that checks whether the agent's coordinate is the same as its goal coordinate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = makeTree({'if': equalFeatureRow(stateKey(actor.name, 'x'), stateKey(actor.name, 'goal_x')),\n",
    "                 True: {'if': equalFeatureRow(stateKey(actor.name, 'y'), stateKey(actor.name, 'goal_y')),\n",
    "                        True: True,\n",
    "                        False: False},\n",
    "                 False: False})\n",
    "world.addTermination(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Turn Order\n",
    "In scenarios with multiple agents, the order in which the agents take turns will matter. If all the agents take action at the same time, use parallel. If the agents take turns one after another, then use sequential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parallel action\n",
    "# world.setOrder([set(world.agents.keys())])\n",
    "# Sequential action\n",
    "world.setOrder(world.agents.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the scenario until termination. At each time step, we can also print the reward for each of the agent's actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%\n",
      "Agent-MoveRight\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveLeft) = -49.000\n",
      "\tV(Agent-MoveUp) = -41.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveDown) = -43.000\n",
      "\tV(Agent-MoveUp) = -37.000\n",
      "100%\n",
      "Agent-MoveRight\n",
      "\tV(Agent-MoveDown) = -39.000\n",
      "\tV(Agent-MoveRight) = -35.000\n",
      "100%\n",
      "Agent-MoveRight\n",
      "\tV(Agent-MoveLeft) = -35.000\n",
      "\tV(Agent-MoveRight) = -33.000\n",
      "100%\n",
      "Agent-MoveDown\n",
      "\tV(Agent-MoveDown) = -33.000\n",
      "\tV(Agent-MoveLeft) = -33.000\n",
      "100%\n",
      "Agent-MoveDown\n",
      "\tV(Agent-MoveDown) = -31.000\n",
      "\tV(Agent-MoveUp) = -33.000\n",
      "100%\n",
      "Agent-MoveRight\n",
      "\tV(Agent-MoveRight) = -27.000\n",
      "\tV(Agent-MoveUp) = -35.000\n",
      "100%\n",
      "Agent-MoveRight\n",
      "\tV(Agent-MoveLeft) = -31.000\n",
      "\tV(Agent-MoveRight) = -21.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveLeft) = -25.000\n",
      "\tV(Agent-MoveUp) = -15.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveDown) = -19.000\n",
      "\tV(Agent-MoveUp) = -10.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveDown) = -13.000\n",
      "\tV(Agent-MoveUp) = -6.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveDown) = -8.000\n",
      "\tV(Agent-MoveUp) = -3.000\n",
      "100%\n",
      "Agent-MoveUp\n",
      "\tV(Agent-MoveDown) = -4.000\n",
      "\tV(Agent-MoveLeft) = -4.000\n",
      "\tV(Agent-MoveUp) = -1.000\n"
     ]
    }
   ],
   "source": [
    "while not world.terminated():\n",
    "    result = world.step()\n",
    "    world.explain(result,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"files/demo.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
